<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Scaling Offline Model-Based RL via Jointly-Optimized World-Action Model Pretraining</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>

  <style>
    .gallery {
      width: 100%;
      max-width: 900px;
      margin: 0 auto;
    }

    .row {
        display: flex;
        justify-content: space-between;
        margin-bottom: 5px;
    }

    .row img {
        width: 19.5%;
        height: auto;
    }


    .container.left-text-right-image {
        display: flex;
        align-items: center;
        max-width: 1200px;
        margin: 0 auto;
    }
    .left-text-right-image .text-content {
        flex: 7;
        padding-right: 25px;
        padding-left: 25px;
    }
    .left-text-right-image .image-container {
        flex: 3;
    }
    .left-text-right-image img {
        height: auto;
        width: auto;
        max-width: 100%;
        max-height: 250px;
    }
</style>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Scaling Offline Model-Based RL via Jointly-Optimized World-Action Model Pretraining</h1>
          <div class="is-size-5 publication-authors">
            <!-- Paper authors -->
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=IOiro9MAAAAJ&hl=zh-CN" target="_blank">Jie Cheng</a><sup>1,2</sup>,
            </span>
            <span class="author-block">
              Ruixi Qiao<sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?hl=zh-CN&user=F4ypDHIAAAAJ" target="_blank">Gang Xiong</a><sup>1,2</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?hl=zh-CN&user=Q4B36ucAAAAJ" target="_blank">Qinghai Miao</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?hl=zh-CN&user=ygjCiRAAAAAJ" target="_blank">Yingwei Ma</a><sup>3</sup>,
            </span>
            <span class="author-block">
              Binhua Li<sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?hl=zh-CN&user=xF5VrokAAAAJ" target="_blank">Yongbin Li</a><sup>3*</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?hl=zh-CN&user=RRKqjKAAAAAJ" target="_blank">Yisheng Lv</a><sup>3*</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <sup>1</sup>State Key Laboratory of Multimodal Artificial Intelligence Systems, CASIA<br>
              <sup>2</sup>Artificial Intelligence, University of Chinese Academy of Sciences<br>
              <sup>3</sup>Alibaba Group
            </span>
            <span class="eql-cntrb"><small><br><sup>*</sup>Corresponding Authors</small></span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- Arxiv PDF link -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2410.00564" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="fas fa-file-pdf"></i>
                </span>
                <span>Paper</span>
                </a>
              </span>

              <!-- Github link -->
              <span class="link-block">
                <a href="https://github.com/CJReinforce/JOWA" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="fab fa-github"></i>
                </span>
                <span>Code</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <h2 class="subtitle has-text-centered">
    <b>TL;DR:</b> A single JOWA-150M agent masters 15 Atari games at 84.7% human-level <br>
    and 119.5% DQN-level, and can adapt to novel games with ~4 expert demos. 
  </h2>
  <div class="gallery">
    <div class="row">
      <img src="static/images/Assault.gif" alt="Assault">
      <img src="static/images/Atlantis.gif" alt="Atlantis">
      <img src="static/images/Carnival.gif" alt="Carnival">
      <img src="static/images/ChopperCommand.gif" alt="ChopperCommand">
      <img src="static/images/DemonAttack.gif" alt="DemonAttack">
    </div>
    <div class="row">
      <img src="static/images/NameThisGame.gif" alt="NameThisGame">
      <img src="static/images/Seaquest.gif" alt="Seaquest">
      <img src="static/images/SpaceInvaders.gif" alt="SpaceInvaders">
      <img src="static/images/TimePilot.gif" alt="TimePilot">
      <img src="static/images/Zaxxon.gif" alt="Zaxxon">
    </div>
    <p class="note">
      *Training used 84x84 grayscale images. RGB demos shown here for better visualization. 
      For Atlantis, only the first 7 minutes of the 2-hour gameplay are displayed.
    </p><br>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            A significant aspiration of offline reinforcement learning (RL) is to develop a generalist agent with high capabilities from large and heterogeneous datasets. However, prior approaches that scale offline RL either rely heavily on expert trajectories or struggle to generalize to diverse unseen tasks. Inspired by the excellent generalization of world model in conditional video generation, we explore the potential of image observation-based world model for scaling offline RL and enhancing generalization on novel tasks. In this paper, we introduce JOWA: Jointly-Optimized World-Action model, an offline model-based RL agent pretrained on multiple Atari games with 6 billion tokens data to learn general-purpose representation and decision-making ability. Our method jointly optimizes a world-action model through a shared transformer backbone, which stabilize temporal difference learning with large models during pretraining. Moreover, we propose a provably efficient and parallelizable planning algorithm to compensate for the Q-value estimation error and thus search out better policies. Experimental results indicate that our largest agent, with 150 million parameters, achieves 78.9% human-level performance on pretrained games using only 10% subsampled offline data, outperforming existing state-of-the-art large-scale offline RL baselines by 31.6% on averange. Furthermore, JOWA scales favorably with model capacity and can sample-efficiently transfer to novel games using only 5k offline fine-tuning data (approximately 4 trajectories) per game, demonstrating superior generalization.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Method -->
<section class="hero teaser" style="padding-top: 40px;">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Method Overview</h2>
    <div class="hero-body">
      <img src="static/images/JOWA_architecture.png" alt="Architecture of JOWA"/>
      <div class="content has-text-justified">
        <p>
          Architecture of JOWA. We use a shared transformer backbone for both world modeling and Q-value criticism to enable joint optimization. 
          VQ-VAE tokenizes images into visual tokens. 
          The sum of vocabulary embeddings, position embeddings and task embeddings forms the input embeddings space for the transformer backbone.
          Training loss is the weighted sum of supervised prediction loss of the world-part module and conservative distributed TD-loss of action-part module. <br>
        </p>
      </div>
    </div>
    <div class="container left-text-right-image">
      <div class="text-content">
        <p>To compensate for the optimal Q-value estimation error and thus search out better policies, 
          we propose a provably efficient and parallelizable planning algorithm 
          and derive the condition under which the search-based optimal Q-values have a lower upper-bound of error than TD learning-based optimal Q-values. 
          The planning helps optimal inference during evaluation and sample-efficient transfer to novel games. <br><br>
          We first model the process of finding optimal actions within the imagined Markov Decision Process as a tree search problem, 
          and then extend beam search as the practical and parallelizable planning algorithm.
          When both the beam width K and horizon H equal to 2, the process of planning is shown in the right figure:</p>
      </div>
      <div class="image-container">
        <img src="static/images/planning.png" alt="Planning algorithm"/>
      </div>
    </div>
  </div>
</section><br>
<!-- End Method -->


<!-- Experiment -->
<section class="hero is-small is-light" style="padding-top: 40px;">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Experiments</h2>
    <div class="hero-body">
      <h3 class="title is-4">Scaling trends</h3>
      <img src="static/images/scaling_trend.png" alt="scaling trend"/>
      <div class="content has-text-justified">
        <p>
          We investigate algorithms' ability to leverage higher capacity architectures.
          The performance of JOWA with planning reliably increases as the model size grows 
          and exhibits the steepest scaling curve among all algorithms.
          Moreover, the proposed planning algorithm improve the performance by a large margin,
          highlighting the great scalability potential of offline model-based RL.
        </p>
      </div>

      <h3 class="title is-4">Few-shot fine-tuning</h3>
      <img src="static/images/fine_tune.png" alt="fine tune"/>
      <div class="content has-text-justified">
        <p>
          We fine-tune pretrained agents on 5 held-out games 
          using uniformly subsampled 5k expert-level transitions 
          (from last 20% of DQN-Replay) per game as the benchmark. 
          These tiny amounts of transitions corresponding to 
          approximately 4 trajectories from expert-level DQN-Replay per fine-tuned game on average, 
          which is similar to the settings of few-shot learning and extremely challenging. <br><br>

          The fine-tuned JOWA-150M attains 64.7% IQM DQN-normalized score across 5 held-out games, 
          outperforming baselines by 34.7% on averange. 
          These results underscore JOWA's capacity for rapid and sample-efficient transfer to novel games, 
          highlighting the efficacy of its learned general-purpose representation and decision-making capabilities.
        </p>
      </div>

      <h3 class="title is-4">Ablation of training choices</h3>
      <img src="static/images/ablation.png" alt="ablation"/>
      <div class="content has-text-justified">
        <p>
          we conduct a series of controlled ablation studies to evaluate the impact of key design choices in JOWA 
          and wish to offer valuable insights for future research in this domain.
          For time-saving, we consider a subset of 6 games in the experiments.
          We train all models for 1M gradient steps and fix the parameter size to 40M.
          The conclusions are summarized as follows: <br><br>
          (i) Joint optimization and distributional TD-loss are crucial for JOWA; <br>
          (ii) CQL regularization, world model for planning, and task embedding significantly improves the performance; <br>
          (iii) We have not observed significant improvements of multiple Q-heads over single Q-head, both for equal weight and random weight summation; <br>
          (iv) Using synthetic data generated by the world model in the pre-training phase hurts performance.
        </p>
      </div>
    </div>
  </div>
</section>
<!-- End Experiment -->


<!--BibTex citation -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{cheng2024scaling,
    title={Scaling Offline Model-Based RL via Jointly-Optimized World-Action Model Pretraining},
    author={Cheng, Jie and Qiao, Ruixi and Xiong, Gang and Miao, Qinghai and Ma, Yingwei and Li, Binhua and Li, Yongbin and Lv, Yisheng},
    journal={arXiv preprint arXiv:2410.00564},
    year={2024}
}</code></pre>
  </div>
</section>
<!--End BibTex citation -->


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

<!-- End of Statcounter Code -->

</body>
</html>
